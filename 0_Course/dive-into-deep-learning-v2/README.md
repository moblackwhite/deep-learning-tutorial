# Related-Information

course home：https://courses.d2l.ai/zh-v2/

textbook：https://zh-v2.d2l.ai/

official github: https://github.com/d2l-ai/d2l-zh


# Chapter

## Ch4. Linear Neural Networks for Classification
### 4.2 The Image Classification

## Ch6. Builders' Guide
### 6.1 Layers and Modules
### 6.2 Parameter Access

[//]: # (### 6.3 Parameter Management)

## Ch7. Convolution Neural Networks
### 7.2 Convolutions for Images
### 7.3 Padding and Stride
### 7.4 Multiple Input and Multiple Output Channels
### 7.5 Pooling

## Ch8. Modern Convolutional Neural Networks
### 8.1 Deep Convolutional Neural Networks (AlexNet)
### 8.2 Network Using Blocks (VGG)
### 8.3 Network in Network (NiN)
https://arxiv.org/abs/1312.4400
### 8.4 Multi-Branch Networks (GoogLeNet)
### 8.5 Batch Normalization
### 8.6
https://arxiv.org/abs/1512.03385


## Ch10. Modern Recurrent Neural Networks
### 10.1 Long Short-Term Memory
### 10.2 Gated Recurrent Units
### 10.3 Deep Recurrent Neural Networks
### 10.5 Machine Translation and the Dataset
### 10.6 The Encoder-Decoder Architecture
### 10.7 Sequence to Sequence Learning for Machine Translation

## Ch11. Attention Mechanisms and Transformers
### 11.1 nadaraya-waston
### 11.3 Attention Scoring Functions
### 11.4 The Bahdanau Attention Mechanism
### 11.5 Multi-Head Attention
### 11.6 Self-Attention and Position
### 11.7 The Transformer Architecture

## Ch14. Computer Vision
### 14.1 Image Augmentation
### 14.2 Fine Tuning
### 14.4 Anchor Boxes
### 14.6 The Object Detection Data

## Ch15 Natural Language Processing: Pretraining
### 15.8 Bidirectional Encoder Representations from Transformers

## Ch16 Natural Language Processing: Applications
### 16.4 Natural Language Inference and the Dataset
### 16.7 Natural Language Inference Fine-Tuning BERT

## Ch20 Generative Adversarial Networks
### 20.2 Deep Convolutional Generative Adversarial Networks
